{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flax_basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4WBy4mQKMS6frPDJVLuQQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deterministic-algorithms-lab/Jax-Journey/blob/main/flax_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twYeooj3pMsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c97bb2b-5ed1-492c-905f-bef3a7c143a5"
      },
      "source": [
        "!pip install --upgrade -q pip jax jaxlib\n",
        "# Install Flax at head:\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5MB 5.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 522kB 36.3MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi5V7X-SofKn"
      },
      "source": [
        "import jax\n",
        "from typing import Any, Callable, Sequence, Optional\n",
        "from jax import lax, random, numpy as jnp\n",
        "import flax\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn\n",
        "\n",
        "from jax.config import config\n",
        "config.enable_omnistaging() # Linen requires enabling omnistaging"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVWgTDepoi_9"
      },
      "source": [
        "* Class attributes are attributes of class specified outside any function. \n",
        "* They are same for all instances of the class.\n",
        "* In below syntax, ```features``` is not a class attribute. In the ```__init__()``` of parent class,  it will be initialized. It is different for different objects, and must be provided during creation of object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-ewBn6KpArI"
      },
      "source": [
        "class ExplicitMLP(nn.Module):\n",
        "    features: Sequence[int]\n",
        "\n",
        "    def setup(self):\n",
        "        '''\n",
        "        This function is called automatically after __postinit__() function. \n",
        "        Here we can register submodules, variables, parameters you will need in your model.\n",
        "        '''\n",
        "        self.layers = [nn.Dense(feat) for feat in self.features]\n",
        "        \n",
        "    def __call__(self, inputs):\n",
        "        '''\n",
        "        Is called whenever inputs are sent in the model.apply()\n",
        "        It doesn't matter whether inputs contain params or not. Don't think about it.\n",
        "        This function just need specifies the flow.\n",
        "        '''\n",
        "        x = inputs\n",
        "        for i, lyr in enumerate(self.layers):\n",
        "            x = lyr(x)\n",
        "            if i!=len(self.layers)-1:\n",
        "                x = nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT9o9scrzDAI"
      },
      "source": [
        "* In the above class, ```model.layers``` won't be accessible from outside the class. It seems like these layers come into existence only when ```model.apply()``` is called.\n",
        "\n",
        "* Below is an example of a neat trick done by flax. If you would like to modify/define the initialisation procedure for a module, at first sight it looks like you will have to pass in and maintain what method to use outside of class(like with ```params```). But, what flax does is that it recognizes that the initialisation method is basically just a combination of function and a random key, so, it will allow you to store and maintain the function part inside the class! (You can do so for functions, but not for shared state.) And this function will take the random key+ shapes etc. as its input and produce deterministic output based on that, which will be used to provide the initial parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf5t9bejp32H"
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "key1, key2 = random.split(key, 2)\n",
        "x = random.normal(key1, (4,4))                                                  #First dimension will automatically be interpretted as batch-dimension. No need to use vmap.\n",
        "\n",
        "model = ExplicitMLP(features=[3,4,5])\n",
        "params = model.init(key2, x)                                                    #Would go on init-ing all the internal layers too."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jZXovvJ6a-W"
      },
      "source": [
        "The ```model.apply()``` below, would have to call each of its sub-layer as specified in ```__call__``` function above. Before calling each of it's sub layers, it sets that specific layer's params properly and would also set various flags that would make sure that you can only use ```__call__``` from inside ```model.apply()``` or ```model.init()```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTEtLzQ4p7Lm",
        "outputId": "0d4ae6dc-38fa-43e1-d2df-de3981eff66d"
      },
      "source": [
        "y = model.apply(params, x)                                                      #Can't do y = model((params,x))\n",
        "\n",
        "print('initialized parameter shapes:\\n', jax.tree_map(jnp.shape, unfreeze(params)))\n",
        "print('output shape:\\n', y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialized parameter shapes:\n",
            " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
            "output shape:\n",
            " (4, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwVwTH5q61CD"
      },
      "source": [
        "Below is another easier method for specifying the flow of steps in the model. We define as well as use the layers directly, specifying only what to pass to it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnro68LUyNdD"
      },
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    features: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        for i, feat in enumerate(self.features):\n",
        "            x = nn.Dense(feat, name=f'layers_{i}')(x)                           #No need to do init/apply etc. as we are in @nn.compact\n",
        "            if i!=len(self.features)-1:\n",
        "                x=nn.relu(x)\n",
        "        return x        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCZOpPAW1c4M",
        "outputId": "dd1bf173-df06-4e3a-f1e1-27d442b862c0"
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "key1, key2 = random.split(key, 2)\n",
        "x = random.uniform(key1, (4,4))\n",
        "\n",
        "model = SimpleMLP([4, 3, 5])\n",
        "params = model.init(key2,x)\n",
        "y = model.apply(params, x)\n",
        "\n",
        "print('initialised parameter shapes:\\n', jax.tree_map(jnp.shape, unfreeze(params)))\n",
        "print('output shape:\\n', y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialised parameter shapes:\n",
            " {'params': {'layers_0': {'bias': (4,), 'kernel': (4, 4)}, 'layers_1': {'bias': (3,), 'kernel': (4, 3)}, 'layers_2': {'bias': (5,), 'kernel': (3, 5)}}}\n",
            "output shape:\n",
            " (4, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_n3I4ie-EEk"
      },
      "source": [
        "Compact notation for defining computation models from scratch, using mathematical operations(only) alongside defining any parameters that the model has. The ```self.param()``` behave differently based on whether ```__call__``` has been called by ```init()``` or ```apply()```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM-yvsTp3kLO"
      },
      "source": [
        "class SimpleDense(nn.Module):\n",
        "    features: int\n",
        "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
        "    bias_init: Callable = nn.initializers.zeros\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        kernel = self.param('kernel',\n",
        "                            self.kernel_init,\n",
        "                            (inputs.shape[-1], self.features))\n",
        "        y = jnp.dot(inputs, kernel)\n",
        "        bias = self.param('bias',\n",
        "                          self.bias_init,\n",
        "                          (self.features, ))\n",
        "        y = y+bias\n",
        "        return y        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB8JzfOk8uqz",
        "outputId": "70c70680-034a-4705-b890-c29456bdbe3a"
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "key1, key2 = random.split(key, 2)\n",
        "x = random.uniform(key1, (4,4))\n",
        "\n",
        "model = SimpleDense(features=3)\n",
        "params = model.init(key2, x)\n",
        "y = model.apply(params, x)\n",
        "\n",
        "print('initialised parameter shapes:\\n', jax.tree_map(jnp.shape, unfreeze(params)))\n",
        "print('output shape:\\n', y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialised parameter shapes:\n",
            " {'params': {'bias': (3,), 'kernel': (4, 3)}}\n",
            "output shape:\n",
            " (4, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzhNWbVQ_A2e"
      },
      "source": [
        "If the above model is implemented using ```setup()``` way, it won't be able to fill in the blank below as no input is available in ```setup()``` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSqCyMcC9Nff"
      },
      "source": [
        "class SimpleDense(nn.Module):\n",
        "    features: int\n",
        "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
        "    bias_init: Callable = nn.initializers.zeros\n",
        "\n",
        "    def setup(self):\n",
        "        self.kernel = self.param('kernel',\n",
        "                                self.kernel_init,\n",
        "                                (___________, self.features))\n",
        "        bias = self.param('bias',\n",
        "                          self.bias_init,\n",
        "                          (self.features, ))\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        y = jnp.dot(inputs, self.kernel)+self.bias\n",
        "        return y        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5humUngRBYN1"
      },
      "source": [
        "* Following code shows how to define variables for a model, apart from its parameters. \n",
        "* The variables, like parameters, are stored in a tree. \n",
        "* And like parameters, are handled outside the class.\n",
        "* To define a variable, specify the entire path from root to the final variable. Here we have specified ```('batch_stats', 'mean')```.\n",
        "* Due to ```@nn.compact()``` the variables and parameters are only initalised and defined once. but all the operations specified are performed every time ```model.apply()``` is called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cRejY3wBT-l"
      },
      "source": [
        "class BiasAdderWithRunningMean(nn.Module):\n",
        "    decay: float = 0.99\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        is_initialized = self.has_variable('batch_stats', 'mean')\n",
        "        ra_mean = self.variable('batch_stats', 'mean',                          #variable entire path name\n",
        "                                lambda s: jnp.zeros(s),                         #initialization function\n",
        "                                x.shape[1:])                                    #input to initialization function\n",
        "        mean = ra_mean.value\n",
        "        bias = self.param('bias', \n",
        "                          lambda rng, shape : jnp.zeros(shape),                 #Since it's a parameter, its lambda function must take rng and shape both. \n",
        "                          x.shape[1:])\n",
        "        \n",
        "        if is_initialized:\n",
        "            ra_mean.value = self.decay * ra_mean.value\\\n",
        "                            + (1.0-self.decay)*jnp.mean(x, axis=0, keepdims=True)\n",
        "\n",
        "        return x - ra_mean.value + bias   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lXEgMugKHDF"
      },
      "source": [
        "* The ```model.apply()``` call has been modified below. You must specify the mutable parameters of the model, and receive them in the output. \n",
        "\n",
        "* The variable ```y``` still contains, the value returned by the ```__call__``` function defined above.\n",
        "\n",
        "* ```model.init()``` returns all the initialized parameters, i.e., variables and params, both. All those are sent into the ```apply()``` call. (And hence they don't need to be initialised again in ```__call__```. )\n",
        "\n",
        "* Although the ```model.apply()``` returns updated variables, but still ```params_n_variables``` has the same old variables. As variables need to be handled outside the class too; so the variables in the ```params_n_variables``` need to be updated here too. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un7GMwwbIIok"
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "key1, key2 = random.split(key, 2)\n",
        "x = random.uniform(key1, (5,))\n",
        "\n",
        "model = BiasAdderWithRunningMean(decay=0.99)\n",
        "params_n_variables = model.init(key2, x)\n",
        "print(params_n_variables)\n",
        "\n",
        "for i in range(10):\n",
        "    x = random.normal(key2+i, (5,))\n",
        "    \n",
        "    y, updated_variables = model.apply(params_n_variables, x, mutable=['batch_stats'])\n",
        "\n",
        "    old_variables, params = params_n_variables.pop('params')                                #remaining tree is first output and popped part(params) is the second\n",
        "    params_n_variables = freeze({'params':params, **updated_variables})                     #New tree being made from the available components\n",
        "    \n",
        "    print(updated_state)\n",
        "\n",
        "print('initialised parameter shapes:\\n', jax.tree_map(jnp.shape, unfreeze(params)))\n",
        "print('output shape:\\n', y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-bhdiKjUagA"
      },
      "source": [
        "#Optimizers in flax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBK-P4S5VyzV"
      },
      "source": [
        "The parameters of the model are stored in the optimizer and are available in ```optimizer.target``` ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-ivwsUbJTcY"
      },
      "source": [
        "from flax import optim\n",
        "optimizer_def = optim.GradientDescent(learning_rate=0.01)\n",
        "optimizer = optimizer_def.create(params)                                        #These params are stored within the class of optimizer and need not be handled outside.\n",
        "loss_grad_fn = jax.value_and_grad(loss)                                         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quwD4QjRU79N"
      },
      "source": [
        "for i in range(101):\n",
        "    loss_val, grad = loss_grad_fn(optimizer.target)\n",
        "    optimizer = optimizer.apply_gradient(grad)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}